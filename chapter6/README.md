# DEAP LEARING
# 6. Создание первой глубокой нейронной сети: введение в обратное распространение

### Задача о светофоре.
###### Эта простая задача поможет нам узнать, как обучаются сети на наборах данных
##### Подготовка данных
###### Нейронные сети не умеют распознавать сигналы светофора

### Матрицы и матричные отношения.
##### Преобразование сигналов светофора в цифровое представление
##### Хорошие матрицы данных идеально имитируют реальный мир
##### Обе матрицы, А и В, представляют один и тот же шаблон
##### Создание матриц в Python
###### Импорт матриц в Python
##### Создание нейронной сети
##### Обучение на полном наборе данных
###### Мы обучили нейронную сеть распознавать одну комбинацию сигналов, но нам нужно, чтобы она распознавала все комбинации


### Полный, пакетный и стохастический градиентный спуск.
##### Стохастический градиентный спуск корректирует сразу все веса для одного примера
##### (Полный) градиентный спуск корректирует веса сразу для одного набора данных
##### Пакетный градиентный спуск корректирует веса после просмотра п примеров


### Нейронные сети изучают корреляцию.
##### Чему обучилась предыдущая нейронная сеть?
##### Повышающее и понижающее давление
###### Давление обусловлено данными
##### Пограничный случай: переобучение
###### Иногда корреляция возникает случайно
##### Пограничный случай: конфликт давлений
###### Иногда корреляция вступает в противоречие сама с собой
###### Так случается не всегда
##### Определение косвенной корреляции
###### Если в ваших данных отсутствует явная корреляция, сгенерируйте промежуточные данные, в которых такая корреляция имеется!
##### Создание корреляции
##### Объединение нейронных сетей в стек: обзор
###### В главе 3 кратко упоминалась возможность комбинирования нейронных сетей. Посмотрим, как это делается

##### Обратное распространение: как это работает?
###### Взвешенное среднее разности


### Переобучение.


### Определение собственной корреляции.


### Обратное распространение: определение причин ошибок на расстоянии.
##### Взвешенная средняя ошибка

### Линейность и нелинейность.
##### Это, пожалуй, самая сложная тема в книге. Не будем спешить здесь
##### Почему составная нейронная сеть не работает
###### Если попробовать обучить трехслойную нейронную сеть в текущем ее виде, она не сойдется


### Иногда корреляция может быть тайной.


### Ваша первая глубокая сеть.
##### Вот как можно получить прогноз


### Обратное распространение в коде: собираем все воедино.
##### Мы можем узнать величину вклада каждой взвешенной суммы в окончательную ошибку

### Объединяем все вместе
##### Вот пример готовой программы, которую вы можете запустить (результат ее работы приводится ниже)
```python
import numpy as np

np.random.seed(1)
def relu(x: int):
    return (x > 0) * x

def relu2deriv(output: int):
    return output > 0

streetlights = np.array([
    [ 1, 0, 1 ],
    [ 0, 1, 1 ],
    [ 0, 0, 1 ],
    [ 1 ,1, 1 ]
])
walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T
alpha = 0.2 
hidden_size = 4
weights_0_1 = 2 * np.random.random((3, hidden_size)) - 1 
weights_1_2 = 2 * np.random.random((hidden_size, 1)) - 1
for iteration in range(60): 
    layer_2_error = 0
    for i in range(len(streetlights)):
        layer_0 = streetlights[i:i+1]
        layer_1 = relu(np.dot(layer_0, weights_0_1)) 
        layer_2 = np.dot(layer_1, weights_1_2)
        layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)
        layer_2_delta = (layer_2 - walk_vs_stop[i:i+1]) 
        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)
        weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta) 
        weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)
    if(iteration % 10 == 9):
        print("Error:" + str(layer_2_error))
```

## Почему глубокие сети важны для нас?
#### Зачем генерировать «промежуточные наборы данных», имеющие корреляцию?